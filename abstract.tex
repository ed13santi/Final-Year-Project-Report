\begin{abstract} 

The existence of prohibitely large state spaces is a common problem of reinforcement learning algorithms in practical domains.
This project proposes state-decomposition approaches to alleviate this problem by allowing more efficient learning in deep
reinforcement learning algorithms. This exploits the almost independence of sub-systems within a large system that only rarely
interact with each other. The developed state-decomposition methods rely on learning sub-agents for each of these sub-systems.
This report focuses on grouping the states into subspaces which have low probability of transitions between each other. The
developed algorithms are tested in "toy" environments and their performance was compared with a baseline Deep Q-learning
algorithm. Amongst the developed techniques, two demonstrated better performance than the baseline algorithm. The algorithm named
'DeltaSwitch' achieved performance gains in both perfectly decomposable environments (perfectly separable into sub-systems) and
more generic environments. The performance gain disappears once the sub-systems are not independent enough. Another algorithm that
achieved performance gains is based on "progressive networks". These techniques were thus demonstrated capable of improving the
efficiency of learning and a possible application of these techniques includes the allocation of processing jobs in inter-connected
data-centres. Further studies can focus on the use of different state-decompositions, not based on transition probabilities and
on techniques to share common information between similar subspaces.

\end{abstract}

\cleardoublepage