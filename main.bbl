\begin{thebibliography}{10}

\bibitem{noauthor_cloud_nodate}
Cloud tensor processing units ({TPUs}).

\bibitem{noauthor_google_nodate}
Google {AI} blog: Scalable deep reinforcement learning for robotic
  manipulation.

\bibitem{noauthor_markov_2021}
Markov decision process.
\newblock Page Version {ID}: 1002665350.

\bibitem{noauthor_openai_nodate}
{OpenAI} gym: the {CartPole}-v0 environment.

\bibitem{noauthor_tensorflow_nodate}
{TensorFlow}.

\bibitem{aggregationHierarchical}
Mehran Asadi and Manfred Huber.
\newblock State space reduction for hierarchical reinforcement learning.
\newblock 2004.

\bibitem{autonomousDriving}
Ravi B~Kiran, Ibrahim Sobh, Talpaert Victor, Patrick Mannion, Ahmad~A.
  Al~Sallab, Senthil Yogamani, and Patrick Pérez.
\newblock Deep reinforcement learning for autonomous driving: A survey.
\newblock {\em arXiv.org}, Feb 2020.

\bibitem{barto_recent_2003}
Andrew~G. Barto and Sridhar Mahadevan.
\newblock Recent advances in hierarchical reinforcement learning.
\newblock 13(1):41--77.

\bibitem{bellman_dynamic_1966}
R.~Bellman.
\newblock Dynamic programming.
\newblock 153(3731):34--37.

\bibitem{stateAggregation}
Thomas~L. {Dean}, Robert {Givan}, and Sonia {Leach}.
\newblock {Model Reduction Techniques for Computing Approximately Optimal
  Solutions for Markov Decision Processes}.
\newblock {\em arXiv e-prints}, page arXiv:1302.1533, February 2013.

\bibitem{dietterich_hierarchical_1999}
Thomas~G. Dietterich.
\newblock Hierarchical reinforcement learning with the {MAXQ} value function
  decomposition.

\bibitem{goos_q-learning_1999}
Chris Gaskett, David Wettergreen, and Alexander Zelinsky.
\newblock Q-learning in continuous state and action spaces.
\newblock In Norman Foo, editor, {\em Advanced Topics in Artificial
  Intelligence}, volume 1747, pages 417--428. Springer Berlin Heidelberg.
\newblock Series Title: Lecture Notes in Computer Science.

\bibitem{worldModels}
David {Ha} and J{\"u}rgen {Schmidhuber}.
\newblock {World Models}.
\newblock {\em arXiv e-prints}, page arXiv:1803.10122, March 2018.

\bibitem{kingma_adam_2017}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.

\bibitem{menache_q-cutdynamic_2002}
Ishai Menache, Shie Mannor, and Nahum Shimkin.
\newblock Q-cut—dynamic discovery of sub-goals in reinforcement learning.
\newblock In Tapio Elomaa, Heikki Mannila, and Hannu Toivonen, editors, {\em
  Machine Learning: {ECML} 2002}, Lecture Notes in Computer Science, pages
  295--306. Springer.

\bibitem{dqn}
Volodymyr {Mnih}, Koray {Kavukcuoglu}, David {Silver}, Alex {Graves}, Ioannis
  {Antonoglou}, Daan {Wierstra}, and Martin {Riedmiller}.
\newblock {Playing Atari with Deep Reinforcement Learning}.
\newblock {\em arXiv e-prints}, page arXiv:1312.5602, December 2013.

\bibitem{mnih_human-level_2015}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A. Rusu, Joel Veness,
  Marc~G. Bellemare, Alex Graves, Martin Riedmiller, Andreas~K. Fidjeland,
  Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis
  Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and
  Demis Hassabis.
\newblock Human-level control through deep reinforcement learning.
\newblock 518(7540):529--533.

\bibitem{moerland_model-based_2020}
Thomas~M. Moerland, Joost Broekens, and Catholijn~M. Jonker.
\newblock Model-based reinforcement learning: A survey.

\bibitem{openai_gym_nodate}
{OpenAI}.
\newblock Gym: A toolkit for developing and comparing reinforcement learning
  algorithms.

\bibitem{parr_reinforcement_nodate}
Ronald Parr and Stuart Russell.
\newblock Reinforcement learning with hierarchies of machines.
\newblock page~7.

\bibitem{NLP}
Romain Paulus, Caiming Xiong, and Richard Socher.
\newblock A deep reinforced model for abstractive summarization, November 2017.

\bibitem{stateActionEmbeddings}
Paul~J. {Pritz}, Liang {Ma}, and Kin~K. {Leung}.
\newblock {Jointly-Trained State-Action Embedding for Efficient Reinforcement
  Learning}.
\newblock {\em arXiv e-prints}, page arXiv:2010.04444, October 2020.

\bibitem{rusu_progressive_2016}
Andrei~A. Rusu, Neil~C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James
  Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.
\newblock Progressive neural networks.

\bibitem{sharma_what_2019}
{SAGAR} {SHARMA}.
\newblock What the hell is perceptron?

\bibitem{silver_mastering_2017}
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew
  Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore
  Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis.
\newblock Mastering chess and shogi by self-play with a general reinforcement
  learning algorithm.

\bibitem{sutton_dyna_1991}
Richard~S. Sutton.
\newblock Dyna, an integrated architecture for learning, planning, and
  reacting.
\newblock 2(4):160--163.

\bibitem{sutton_barto_2018}
Richard~S. Sutton and Andrew~G. Barto.
\newblock {\em Reinforcement learning: an introduction}.
\newblock The MIT Press, 2018.

\bibitem{sutton_between_1999}
Richard~S. Sutton, Doina Precup, and Satinder Singh.
\newblock Between {MDPs} and semi-{MDPs}: A framework for temporal abstraction
  in reinforcement learning.
\newblock 112(1):181--211.

\bibitem{keras}
Keras Team.
\newblock Simple. flexible. powerful.
\newblock Accessed: 2021-01-23.

\bibitem{van_hasselt_deep_2015}
Hado van Hasselt, Arthur Guez, and David Silver.
\newblock Deep reinforcement learning with double q-learning.

\bibitem{wang_dueling_nodate}
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando
  de~Freitas.
\newblock Dueling network architectures for deep reinforcement learning.
\newblock page~9.

\bibitem{watkins_q-learning_1992}
Christopher J. C.~H. Watkins and Peter Dayan.
\newblock Q-learning.
\newblock 8(3):279--292.

\bibitem{watkins_1989}
Christopher John Cornish~Hellaby. Watkins.
\newblock {\em Learning from delayed rewards}.
\newblock PhD thesis, University of Cambridge, 1989.

\bibitem{wiskunde_doubleq-learning_nodate}
Centrum Wiskunde.
\newblock {\em {DoubleQ}-learning Hado van Hasselt Multi-agent and Adaptive
  Computation Group}.

\bibitem{yu_reinforcement_2020}
Chao Yu, Jiming Liu, and Shamim Nemati.
\newblock Reinforcement learning in healthcare: A survey.

\bibitem{ServicePlacement}
Ziyao Zhang, Liang Ma, Kin~K. Leung, Leandros Tassiulas, and Jeremy Tucker.
\newblock Q-placement: Reinforcement-learning-based service placement in
  software-defined networks.
\newblock {\em 2018 IEEE 38th International Conference on Distributed Computing
  Systems (ICDCS)}, 2018.

\bibitem{SDNSynchronisation}
Ziyao Zhang, Liang Ma, Konstantinos Poularakis, Kin~K. Leung, and Lingfei Wu.
\newblock Dq scheduler: Deep reinforcement learning based controller
  synchronization in distributed sdn.
\newblock {\em ICC 2019 - 2019 IEEE International Conference on Communications
  (ICC)}, 2019.

\bibitem{simsek_identifying_2005}
Özgür Şimşek, Alicia~P. Wolfe, and Andrew~G. Barto.
\newblock Identifying useful subgoals in reinforcement learning by local graph
  partitioning.
\newblock In {\em Proceedings of the 22nd international conference on Machine
  learning}, {ICML} '05, pages 816--823. Association for Computing Machinery.

\end{thebibliography}
