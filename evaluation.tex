\chapter{Applicability of the decomposition method}

\paragraph{} The previous chapter shows that given certain conditions, the state-decomposition method can give a performance gain
in terms of the rewards obtained during the learning process. This chapter aims to provide a practical example where
state-decomposition could be applied to a reinforcement learning solution and could achieve superior performance compared to a
single global agent such as a standard DQN agent.

\paragraph{} In general, techniques based on the state-decomposition method could be useful for any kind of RL problem in which
using multiple function approximators to estimate the action-values allows to learn simpler functions than using a single
approximator. This is often the case in systems composed of multiple sub-systems, where the various components seldom interact with
each other, which is the case in which the originally proposed state-decomposition technique can be used. It is useful to discuss
a possible application of the proposed methodology to better show how such a technique could be applied to a real-life scenario.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\linewidth]{figures/dataCentres.pdf}
    \caption{Allocation of resources problem for two interconnected data centres.}
    \label{fig:data-centres}
\end{figure}

\paragraph{} Consider a network scenario with two data centres, each of which has multiple computers, as shown in Figure
\ref{fig:data-centres}. Each of these data centres receives processing tasks (jobs) that can be assigned to the computers and
completed. Each of these jobs requires a random time to process on its allocated machine. At each time-step there can be no task
or one task arriving at one of the data centres. The resource allocation problem deals with assigning these tasks to machines in
the two data centres in order to minimise the average waiting time to completion of the tasks.  Most of the time it is most
efficient for the data-centre that receives the task to process it locally, as sending the task to the other data-centres incurs
in additional delays. However, sometimes a data-centre is too busy to justify the additional delay incurred in trasferring tasks
between data-centres. This problem can be formulated as a Markov Decision Process and solved using reinforcement learning. The MDP
can be defined as follows:
\begin{itemize}
    \item The state of the MDP indicates the number of tasks waiting for each machine (a computer in a data centre). It also
    indicates the presence of eventual incoming tasks in the data centres. Thus the state can be represented by a vector having an
    element for each machine, which takes integer values indicating the number of jobs waiting for processing in the corresponding
    machine. The vector would also include two additional elements, taking values 0 or 1, indicating the presence of new incoming
    tasks in each of the data centres.
    \item The actions to be taken by the controller are to assign the tasks to the machines. At each time-step, the controller can
    allocate the received task to any machine, either in the local or in the remote data centre.
    \item The rewards can be positive values that occur when tasks are completed. In order to encourage local allocation of tasks,
    the rewards can be higher when task are completed in the data centre that receives them than when they are processed remotely.
\end{itemize}
If the difference of the rewards upon completion of tasks between local and remote data centres is set to be very high, the
controller will allocate most tasks locally. The fact that the controller is unlikely to assign tasks to the remote data centre
makes it so that the equivalent transitions are unlikely to happen and that the state space can be decomposed accordingly. 

\paragraph{} The simplest way of applying state-decomposition in this scenario would be with a 'DeltaSwitch' agent having two
neural networks, one for when a task is received by one data centre and another NN for when a task is received by the other data
centre. Though this might be effective, it does not use the decomposition previously discussed based on decomposing by 'ignoring'
state transitions below a certain threshold. In this case, an intuitive decomposition of treating two relatively separate problems
as such is used.

\paragraph{} In the case where the difference in rewards between completing a task locally or remotely is infinite, the tasks
would always be solved locally. In this case the problem of deciding which data centre the task should be processed does not
exist. In this case we could simply have two separate agents for the two data centres that learn how to assign tasks within the
data centres, without dealing with their interconnection. However, as the difference in rewards decreases, it becomes optimal to
use the interconnection. The 'learn the switch' and techniques discussed in the previous chapter would be useful in this case. The
networks trained to assign tasks internally within the two data-centres can be combined in order to learn a global agent that can
assign tasks in both in the most optimal manner. This architectures is illustrated in
Figure \ref{fig:data-centres-decomposition-agent}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figures/data-centre-decomposition.pdf}
    \caption{Possible state-decomposition agent.}
    \label{fig:data-centres-decomposition-agent}
\end{figure}

\paragraph{} This last methodology is very similar to what would be done in a hierarchical architecture. This would have an agent
that explicitly decides whether a task should be assigned to data centre 1 or 2. Given this knowledge a local agent trained to
decide how to assign the task internally within the data centre would be used and the outputs of these two would be combined to
give the full action description. The difference with the state-decomposition method is that while in the latter the full action
description is given by the combination network and all other components merely provide implicit information to it, in the
hierarchical approach the action description is formed explicitly by combining the outputs of multiple 'layers' of local agents.
Additionally, the hierarchical approach deals with the problem with a 'high level' to 'low level' approach, in which 'high level'
actions are decided first and 'low level' actions are determined within the 'high level' actions, while the state-decomposition
approach feeds the knowledge of the possible 'low level' agents to a global agent that outputs a full action description.

\paragraph{} Thought it was shown that the proposed state-decomposition techniques can be beneficial in terms of sample
efficiency, there is an increase in complexity of the design of the agent, compared to simple DQN. The main issue is that the
state-transition matrix is not normally known in applications that use reinforcement learning, meaning that if the decomposition
is not "intuitive" as in this example, this would have to be estimated before being able to apply state-decomposition.
Additionally, there are cases in which there might be different types of possible decompositions and there is no systematic way to
predict which one is most efficient. Lastly, while it was shown that the state-decomposition techniques achieve performance gain
in certain environment, this is not guaranteed in general, thus the extra design effort might not be rewarded by increased
efficiency is some cases. A list of possible extensions of this project, some of which address these issues, is given in Section
\ref{sec:extensions}.

