
@misc{openai_gym_nodate,
	title = {Gym: {A} toolkit for developing and comparing reinforcement learning algorithms},
	shorttitle = {Gym},
	url = {https://gym.openai.com},
	abstract = {A toolkit for developing and comparing reinforcement learning algorithms},
	urldate = {2021-02-01},
	author = {OpenAI},
}

@misc{noauthor_tensorflow_nodate,
	title = {{TensorFlow}},
	url = {https://www.tensorflow.org/},
	urldate = {2021-02-01},
}

@article{barto_recent_2003,
	title = {Recent {Advances} in {Hierarchical} {Reinforcement} {Learning}},
	volume = {13},
	issn = {1573-7594},
	url = {https://doi.org/10.1023/A:1022140919877},
	doi = {10.1023/A:1022140919877},
	abstract = {Reinforcement learning is bedeviled by the curse of dimensionality: the number of parameters to be learned grows exponentially with the size of any compact encoding of a state. Recent attempts to combat the curse of dimensionality have turned to principled ways of exploiting temporal abstraction, where decisions are not required at each step, but rather invoke the execution of temporally-extended activities which follow their own policies until termination. This leads naturally to hierarchical control architectures and associated learning algorithms. We review several approaches to temporal abstraction and hierarchical organization that machine learning researchers have recently developed. Common to these approaches is a reliance on the theory of semi-Markov decision processes, which we emphasize in our review. We then discuss extensions of these ideas to concurrent activities, multiagent coordination, and hierarchical memory for addressing partial observability. Concluding remarks address open challenges facing the further development of reinforcement learning in a hierarchical setting.},
	language = {en},
	number = {1},
	urldate = {2021-02-01},
	journal = {Discrete Event Dynamic Systems},
	author = {Barto, Andrew G. and Mahadevan, Sridhar},
	month = jan,
	year = {2003},
	pages = {41--77},
}

@misc{noauthor_cloud_nodate,
	title = {Cloud {Tensor} {Processing} {Units} ({TPUs})},
	url = {https://cloud.google.com/tpu/docs/tpus},
	language = {en},
	urldate = {2021-01-29},
	journal = {Google Cloud},
}

@misc{noauthor_openai_nodate,
	title = {{OpenAI} {Gym}: the {CartPole}-v0 environment},
	shorttitle = {{OpenAI} {Gym}},
	url = {https://gym.openai.com/envs/CartPole-v0},
	abstract = {See the scores on all CartPole-v0 evaluations.},
	language = {en},
	urldate = {2021-01-29},
}

@misc{noauthor_markov_2021,
	title = {Markov decision process},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Markov_decision_process&oldid=1002665350},
	abstract = {In mathematics, a Markov decision process (MDP) is a discrete-time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying optimization problems solved via dynamic programming. MDPs were known at least as early as the 1950s; a core body of research on Markov decision processes resulted from Ronald Howard's 1960 book, Dynamic Programming and Markov Processes. They are used in many disciplines, including robotics, automatic control, economics and manufacturing. The name of MDPs comes from the Russian mathematician Andrey Markov as they are an extension of Markov chains.
At each time step, the process is in some state 
  
    
      
        s
      
    
    \{{\textbackslash}displaystyle s\}
  , and the decision maker may choose any action 
  
    
      
        a
      
    
    \{{\textbackslash}displaystyle a\}
   that is available in state 
  
    
      
        s
      
    
    \{{\textbackslash}displaystyle s\}
  . The process responds at the next time step by randomly moving into a new state 
  
    
      
        
          s
          ′
        
      
    
    \{{\textbackslash}displaystyle s'\}
  , and giving the decision maker a corresponding reward 
  
    
      
        
          R
          
            a
          
        
        (
        s
        ,
        
          s
          ′
        
        )
      
    
    \{{\textbackslash}displaystyle R\_\{a\}(s,s')\}
  .
The probability that the process moves into its new state 
  
    
      
        
          s
          ′
        
      
    
    \{{\textbackslash}displaystyle s'\}
   is influenced by the chosen action. Specifically, it is given by the state transition function 
  
    
      
        
          P
          
            a
          
        
        (
        s
        ,
        
          s
          ′
        
        )
      
    
    \{{\textbackslash}displaystyle P\_\{a\}(s,s')\}
  . Thus, the next state 
  
    
      
        
          s
          ′
        
      
    
    \{{\textbackslash}displaystyle s'\}
   depends on the current state 
  
    
      
        s
      
    
    \{{\textbackslash}displaystyle s\}
   and the decision maker's action 
  
    
      
        a
      
    
    \{{\textbackslash}displaystyle a\}
  . But given 
  
    
      
        s
      
    
    \{{\textbackslash}displaystyle s\}
   and 
  
    
      
        a
      
    
    \{{\textbackslash}displaystyle a\}
  , it is conditionally independent of all previous states and actions; in other words, the state transitions of an MDP satisfy the Markov property.
Markov decision processes are an extension of Markov chains; the difference is the addition of actions (allowing choice) and rewards (giving motivation). Conversely, if only one action exists for each state (e.g. "wait") and all rewards are the same (e.g. "zero"), a Markov decision process reduces to a Markov chain.},
	language = {en},
	urldate = {2021-01-29},
	journal = {Wikipedia},
	month = jan,
	year = {2021},
	note = {Page Version ID: 1002665350},
}

@article{sutton_between_1999,
	title = {Between {MDPs} and semi-{MDPs}: {A} framework for temporal abstraction in reinforcement learning},
	volume = {112},
	issn = {0004-3702},
	shorttitle = {Between {MDPs} and semi-{MDPs}},
	url = {http://www.sciencedirect.com/science/article/pii/S0004370299000521},
	doi = {10.1016/S0004-3702(99)00052-1},
	abstract = {Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options—closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options defined over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro-utility problem.},
	language = {en},
	number = {1},
	urldate = {2021-01-29},
	journal = {Artificial Intelligence},
	author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder},
	month = aug,
	year = {1999},
	keywords = {Hierarchical planning, Intra-option learning, Macroactions, Macros, Markov decision processes, Options, Reinforcement learning, Semi-Markov decision processes, Subgoals, Temporal abstraction},
	pages = {181--211},
}

@article{parr_reinforcement_nodate,
	title = {Reinforcement {Learning} with {Hierarchies} of {Machines}},
	abstract = {We present a new approach to reinforcement learning in which the policies considered by the learning process are constrained by hierarchies of partially speciﬁed machines. This allows for the use of prior knowledge to reduce the search space and provides a framework in which knowledge can be transferred across problems and in which component solutions can be recombined to solve larger and more complicated problems. Our approach can be seen as providing a link between reinforcement learning and “behavior-based” or “teleo-reactive” approaches to control. We present provably convergent algorithms for problem-solving and learning with hierarchical machines and demonstrate their effectiveness on a problem with several thousand states.},
	language = {en},
	author = {Parr, Ronald and Russell, Stuart},
	pages = {7},
}

@article{watkins_learning_1989,
	title = {Learning {From} {Delayed} {Rewards}},
	abstract = {Photocopy. Supplied by British Library. Thesis (Ph. D.)--King's College, Cambridge, 1989.},
	author = {Watkins, Christopher},
	month = jan,
	year = {1989},
}

@inproceedings{zhang_q-placement_2018,
	title = {Q-{Placement}: {Reinforcement}-{Learning}-{Based} {Service} {Placement} in {Software}-{Defined} {Networks}},
	shorttitle = {Q-{Placement}},
	doi = {10.1109/ICDCS.2018.00159},
	abstract = {In software-defined networking (SDN) paradigm, where the control and data plane are separated, the scalability of the SDN controller in the control plane is critical and can affect the overall network performance significantly. To improve controller scalability, efforts have been put into enhancing the capability of SDN switches in the data plane, to make them more autonomous in providing routine services without consulting the controller. In this regard, we investigate the service placement problem on SDN switches aiming at minimizing the average accumulated service costs for end users. To solve this problem, we propose a novel reinforcement-learning-based algorithm with guaranteed performance and convergence rate, called Q-placement. Comparing to traditional optimization techniques, Q-placement exhibits many appealing features, such as performance-tuneable optimization and off-the-shelf implementation. Extensive evaluations show that Q-placement consistently outperforms benchmarks and other state-of-the-art algorithms in both synthetic and real networks. Moreover, these evaluations reveal insights into how the network topological properties (e.g., density), servicing capacities, and controller's roles affect the accumulated service costs, which is useful in service planning tasks.},
	booktitle = {2018 {IEEE} 38th {International} {Conference} on {Distributed} {Computing} {Systems} ({ICDCS})},
	author = {Zhang, Z. and Ma, L. and Leung, K. K. and Tassiulas, L. and Tucker, J.},
	month = jul,
	year = {2018},
	note = {ISSN: 2575-8411},
	keywords = {Control systems, Decision making, Electronic mail, Network topology, Optimization, Q-placement, SDN, SDN controller, Scalability, Task analysis, average accumulated service costs, control plane, controller scalability, convergence rate, data plane, learning (artificial intelligence), network performance, network topological properties, optimisation, optimization techniques, performance-tuneable optimization, q learning, real networks, reinforcement learning, reinforcement-learning-based algorithm, reinforcement-learning-based service placement, service placement, service placement problem, service planning tasks, software defined networking, software-defined networking paradigm, synthetic networks, telecommunication network topology},
	pages = {1527--1532},
}

@article{zhang_dq_2018,
	title = {{DQ} {Scheduler}: {Deep} {Reinforcement} {Learning} {Based} {Controller} {Synchronization} in {Distributed} {SDN}},
	shorttitle = {{DQ} {Scheduler}},
	url = {http://arxiv.org/abs/1812.00852},
	abstract = {In distributed software-defined networks (SDN), multiple physical SDN controllers, each managing a network domain, are implemented to balance centralized control, scalability and reliability requirements. In such networking paradigm, controllers synchronize with each other to maintain a logically centralized network view. Despite various proposals of distributed SDN controller architectures, most existing works only assume that such logically centralized network view can be achieved with some synchronization designs, but the question of how exactly controllers should synchronize with each other to maximize the benefits of synchronization under the eventual consistency assumptions is largely overlooked. To this end, we formulate the controller synchronization problem as a Markov Decision Process (MDP) and apply reinforcement learning techniques combined with deep neural network to train a smart controller synchronization policy, which we call the Deep-Q (DQ) Scheduler. Evaluation results show that DQ Scheduler outperforms the antientropy algorithm implemented in the ONOS controller by up to 95.2\% for inter-domain routing tasks.},
	urldate = {2021-01-29},
	journal = {arXiv:1812.00852 [cs]},
	author = {Zhang, Ziyao and Ma, Liang and Poularakis, Konstantinos and Leung, Kin K. and Wu, Lingfei},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.00852},
	keywords = {Computer Science - Networking and Internet Architecture},
}

@article{paulus_deep_2017,
	title = {A {Deep} {Reinforced} {Model} for {Abstractive} {Summarization}},
	url = {https://arxiv.org/abs/1705.04304v3},
	abstract = {Attentional, RNN-based encoder-decoder models for abstractive summarization
have achieved good performance on short input and output sequences. For longer
documents and summaries however these models often include repetitive and
incoherent phrases. We introduce a neural network model with a novel
intra-attention that attends over the input and continuously generated output
separately, and a new training method that combines standard supervised word
prediction and reinforcement learning (RL). Models trained only with supervised
learning often exhibit "exposure bias" - they assume ground truth is provided
at each step during training. However, when standard word prediction is
combined with the global sequence prediction training of RL the resulting
summaries become more readable. We evaluate this model on the CNN/Daily Mail
and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the
CNN/Daily Mail dataset, an improvement over previous state-of-the-art models.
Human evaluation also shows that our model produces higher quality summaries.},
	language = {en},
	urldate = {2021-01-29},
	author = {Paulus, Romain and Xiong, Caiming and Socher, Richard},
	month = may,
	year = {2017},
}

@article{kiran_deep_2021,
	title = {Deep {Reinforcement} {Learning} for {Autonomous} {Driving}: {A} {Survey}},
	shorttitle = {Deep {Reinforcement} {Learning} for {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/2002.00444},
	abstract = {With the development of deep representation learning, the domain of reinforcement learning (RL) has become a powerful learning framework now capable of learning complex policies in high dimensional environments. This review summarises deep reinforcement learning (DRL) algorithms and provides a taxonomy of automated driving tasks where (D)RL methods have been employed, while addressing key computational challenges in real world deployment of autonomous driving agents. It also delineates adjacent domains such as behavior cloning, imitation learning, inverse reinforcement learning that are related but are not classical RL algorithms. The role of simulators in training agents, methods to validate, test and robustify existing solutions in RL are discussed.},
	urldate = {2021-01-29},
	journal = {arXiv:2002.00444 [cs]},
	author = {Kiran, B. Ravi and Sobh, Ibrahim and Talpaert, Victor and Mannion, Patrick and Sallab, Ahmad A. Al and Yogamani, Senthil and Pérez, Patrick},
	month = jan,
	year = {2021},
	note = {arXiv: 2002.00444},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{noauthor_google_nodate,
	title = {Google {AI} {Blog}: {Scalable} {Deep} {Reinforcement} {Learning} for {Robotic} {Manipulation}},
	url = {https://ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html},
	urldate = {2021-01-29},
}

@article{yu_reinforcement_2020,
	title = {Reinforcement {Learning} in {Healthcare}: {A} {Survey}},
	shorttitle = {Reinforcement {Learning} in {Healthcare}},
	url = {http://arxiv.org/abs/1908.08796},
	abstract = {As a subfield of machine learning, reinforcement learning (RL) aims at empowering one's capabilities in behavioural decision making by using interaction experience with the world and an evaluative feedback. Unlike traditional supervised learning methods that usually rely on one-shot, exhaustive and supervised reward signals, RL tackles with sequential decision making problems with sampled, evaluative and delayed feedback simultaneously. Such distinctive features make RL technique a suitable candidate for developing powerful solutions in a variety of healthcare domains, where diagnosing decisions or treatment regimes are usually characterized by a prolonged and sequential procedure. This survey discusses the broad applications of RL techniques in healthcare domains, in order to provide the research community with systematic understanding of theoretical foundations, enabling methods and techniques, existing challenges, and new insights of this emerging paradigm. By first briefly examining theoretical foundations and key techniques in RL research from efficient and representational directions, we then provide an overview of RL applications in healthcare domains ranging from dynamic treatment regimes in chronic diseases and critical care, automated medical diagnosis from both unstructured and structured clinical data, as well as many other control or scheduling domains that have infiltrated many aspects of a healthcare system. Finally, we summarize the challenges and open issues in current research, and point out some potential solutions and directions for future research.},
	urldate = {2021-01-29},
	journal = {arXiv:1908.08796 [cs]},
	author = {Yu, Chao and Liu, Jiming and Nemati, Shamim},
	month = apr,
	year = {2020},
	note = {arXiv: 1908.08796},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{silver_mastering_2017,
	title = {Mastering the game of {Go} without human knowledge},
	volume = {550},
	copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature24270},
	doi = {10.1038/nature24270},
	abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.},
	language = {en},
	number = {7676},
	urldate = {2021-01-29},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	month = oct,
	year = {2017},
	note = {Number: 7676
Publisher: Nature Publishing Group},
	pages = {354--359},
}

@article{sutton_reinforcement_nodate,
	title = {Reinforcement {Learning}: {An} {Introduction}},
	language = {en},
	author = {Sutton, Richard S and Barto, Andrew G},
	pages = {352},
}

@article{watkins_q-learning_1992,
	title = {Q-learning},
	volume = {8},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00992698},
	doi = {10.1007/BF00992698},
	abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
	language = {en},
	number = {3},
	urldate = {2021-01-29},
	journal = {Machine Learning},
	author = {Watkins, Christopher J. C. H. and Dayan, Peter},
	month = may,
	year = {1992},
	pages = {279--292},
}

@article{bellman_dynamic_1966,
	title = {Dynamic {Programming}},
	volume = {153},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.153.3731.34},
	doi = {10.1126/science.153.3731.34},
	language = {en},
	number = {3731},
	urldate = {2021-01-28},
	journal = {Science},
	author = {Bellman, R.},
	month = jul,
	year = {1966},
	pages = {34--37},
}

@article{sutton_dyna_1991,
	title = {Dyna, an integrated architecture for learning, planning, and reacting},
	volume = {2},
	issn = {0163-5719},
	url = {https://doi.org/10.1145/122344.122377},
	doi = {10.1145/122344.122377},
	abstract = {Dyna is an AI architecture that integrates learning, planning, and reactive execution. Learning methods are used in Dyna both for compiling planning results and for updating a model of the effects of the agent's actions on the world. Planning is incremental and can use the probabilistic and ofttimes incorrect world models generated by learning processes. Execution is fully reactive in the sense that no planning intervenes between perception and action. Dyna relies on machine learning methods for learning from examples---these are among the basic building blocks making up the architecture---yet is not tied to any particular method. This paper briefly introduces Dyna and discusses its strengths and weaknesses with respect to other architectures.},
	number = {4},
	urldate = {2021-01-28},
	journal = {ACM SIGART Bulletin},
	author = {Sutton, Richard S.},
	month = jul,
	year = {1991},
	pages = {160--163},
}

@article{silver_mastering_2017-1,
	title = {Mastering {Chess} and {Shogi} by {Self}-{Play} with a {General} {Reinforcement} {Learning} {Algorithm}},
	url = {http://arxiv.org/abs/1712.01815},
	abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
	urldate = {2021-01-28},
	journal = {arXiv:1712.01815 [cs]},
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.01815},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{menache_q-cutdynamic_2002,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Q-{Cut}—{Dynamic} {Discovery} of {Sub}-goals in {Reinforcement} {Learning}},
	isbn = {978-3-540-36755-0},
	doi = {10.1007/3-540-36755-1_25},
	abstract = {We present the Q-Cut algorithm, a graph theoretic approach for automatic detection of sub-goals in a dynamic environment, which is used for acceleration of the Q-Learning algorithm. The learning agent creates an on-line map of the process history, and uses an efficient Max-Flow/Min-Cut algorithm for identifying bottlenecks. The policies for reaching bottlenecks are separately learned and added to the model in a form of options (macro-actions). We then extend the basic Q-Cut algorithm to the Segmented Q-Cut algorithm, which uses previously identified bottlenecks for state space partitioning, necessary for finding additional bottlenecks in complex environments. Experiments show significant performance improvements, particulary in the initial learning phase.},
	language = {en},
	booktitle = {Machine {Learning}: {ECML} 2002},
	publisher = {Springer},
	author = {Menache, Ishai and Mannor, Shie and Shimkin, Nahum},
	editor = {Elomaa, Tapio and Mannila, Heikki and Toivonen, Hannu},
	year = {2002},
	keywords = {Dynamic Discovery, Learning Agent, Markov Decision Process, Reinforcement Learn, Temporal Abstraction},
	pages = {295--306},
}

@inproceedings{simsek_identifying_2005,
	address = {New York, NY, USA},
	series = {{ICML} '05},
	title = {Identifying useful subgoals in reinforcement learning by local graph partitioning},
	isbn = {978-1-59593-180-1},
	url = {https://doi.org/10.1145/1102351.1102454},
	doi = {10.1145/1102351.1102454},
	abstract = {We present a new subgoal-based method for automatically creating useful skills in reinforcement learning. Our method identifies subgoals by partitioning local state transition graphs---those that are constructed using only the most recent experiences of the agent. The local scope of our subgoal discovery method allows it to successfully identify the type of subgoals we seek---states that lie between two densely-connected regions of the state space while producing an algorithm with low computational cost.},
	urldate = {2021-01-28},
	booktitle = {Proceedings of the 22nd international conference on {Machine} learning},
	publisher = {Association for Computing Machinery},
	author = {Şimşek, Ozgür and Wolfe, Alicia P. and Barto, Andrew G.},
	month = aug,
	year = {2005},
	pages = {816--823},
}

@article{dietterich_hierarchical_1999,
	title = {Hierarchical {Reinforcement} {Learning} with the {MAXQ} {Value} {Function} {Decomposition}},
	url = {http://arxiv.org/abs/cs/9905014},
	abstract = {This paper presents the MAXQ approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy of smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The paper defines the MAXQ hierarchy, proves formal results on its representational power, and establishes five conditions for the safe use of state abstractions. The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges wih probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the five kinds of state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q (with state abstractions) converges to a recursively optimal policy much faster than flat Q learning. The fact that MAXQ learns a representation of the value function has an important benefit: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration. The paper demonstrates the effectiveness of this non-hierarchical execution experimentally. Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoffs in hierarchical reinforcement learning.},
	urldate = {2021-01-25},
	journal = {arXiv:cs/9905014},
	author = {Dietterich, Thomas G.},
	month = may,
	year = {1999},
	note = {arXiv: cs/9905014},
	keywords = {Computer Science - Machine Learning, I.2.6},
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	language = {en},
	number = {7540},
	urldate = {2021-01-24},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	pages = {529--533},
}

@article{van_hasselt_deep_2015,
	title = {Deep {Reinforcement} {Learning} with {Double} {Q}-learning},
	url = {http://arxiv.org/abs/1509.06461},
	abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
	urldate = {2021-01-24},
	journal = {arXiv:1509.06461 [cs]},
	author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
	month = dec,
	year = {2015},
	note = {arXiv: 1509.06461},
	keywords = {Computer Science - Machine Learning},
}

@incollection{goos_q-learning_1999,
	address = {Berlin, Heidelberg},
	title = {Q-{Learning} in {Continuous} {State} and {Action} {Spaces}},
	volume = {1747},
	isbn = {978-3-540-66822-0 978-3-540-46695-6},
	url = {http://link.springer.com/10.1007/3-540-46695-9_35},
	abstract = {Q-learning can be used to learn a control policy that maximises a scalar reward through interaction with the environment. Qlearning is commonly applied to problems with discrete states and actions. We describe a method suitable for control tasks which require continuous actions, in response to continuous states. The system consists of a neural network coupled with a novel interpolator. Simulation results are presented for a non-holonomic control task. Advantage Learning, a variation of Q-learning, is shown enhance learning speed and reliability for this task.},
	language = {en},
	urldate = {2021-01-24},
	booktitle = {Advanced {Topics} in {Artificial} {Intelligence}},
	publisher = {Springer Berlin Heidelberg},
	author = {Gaskett, Chris and Wettergreen, David and Zelinsky, Alexander},
	editor = {Goos, G. and Hartmanis, J. and van Leeuwen, J. and Foo, Norman},
	year = {1999},
	doi = {10.1007/3-540-46695-9_35},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {417--428},
}

@book{wiskunde_doubleq-learning_nodate,
	title = {{DoubleQ}-learning {Hado} van {Hasselt} {Multi}-agent and {Adaptive} {Computation} {Group}},
	abstract = {In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alternative way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimalpolicyandthatitperformswellinsomesettingsinwhichQ-learningperformspoorly due toitsoverestimation. 1},
	author = {Wiskunde, Centrum},
}

@article{wang_dueling_nodate,
	title = {Dueling {Network} {Architectures} for {Deep} {Reinforcement} {Learning}},
	abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main beneﬁt of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
	language = {en},
	author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and van Hasselt, Hado and Lanctot, Marc and de Freitas, Nando},
	pages = {9},
}

@article{moerland_model-based_2020,
	title = {Model-based {Reinforcement} {Learning}: {A} {Survey}},
	shorttitle = {Model-based {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2006.16712},
	abstract = {Sequential decision making, commonly formalized as Markov Decision Process (MDP) optimization, is a key challenge in artificial intelligence. Two key approaches to this problem are reinforcement learning (RL) and planning. This paper presents a survey of the integration of both fields, better known as model-based reinforcement learning. Model-based RL has two main steps. First, we systematically cover approaches to dynamics model learning, including challenges like dealing with stochasticity, uncertainty, partial observability, and temporal abstraction. Second, we present a systematic categorization of planning-learning integration, including aspects like: where to start planning, what budgets to allocate to planning and real data collection, how to plan, and how to integrate planning in the learning and acting loop. After these two key sections, we also discuss the potential benefits of model-based RL, like enhanced data efficiency, targeted exploration, and improved stability. Along the survey, we also draw connections to several related RL fields, like hierarchical RL and transfer, and other research disciplines, like behavioural psychology. Altogether, the survey presents a broad conceptual overview of planning-learning combinations for MDP optimization.},
	urldate = {2021-01-24},
	journal = {arXiv:2006.16712 [cs, stat]},
	author = {Moerland, Thomas M. and Broekens, Joost and Jonker, Catholijn M.},
	month = jul,
	year = {2020},
	note = {arXiv: 2006.16712},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}
