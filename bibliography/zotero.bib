
@article{moerland_model-based_2020,
	title = {Model-based Reinforcement Learning: A Survey},
	url = {http://arxiv.org/abs/2006.16712},
	shorttitle = {Model-based Reinforcement Learning},
	abstract = {Sequential decision making, commonly formalized as Markov Decision Process ({MDP}) optimization, is a key challenge in artificial intelligence. Two key approaches to this problem are reinforcement learning ({RL}) and planning. This paper presents a survey of the integration of both fields, better known as model-based reinforcement learning. Model-based {RL} has two main steps. First, we systematically cover approaches to dynamics model learning, including challenges like dealing with stochasticity, uncertainty, partial observability, and temporal abstraction. Second, we present a systematic categorization of planning-learning integration, including aspects like: where to start planning, what budgets to allocate to planning and real data collection, how to plan, and how to integrate planning in the learning and acting loop. After these two key sections, we also discuss the potential benefits of model-based {RL}, like enhanced data efficiency, targeted exploration, and improved stability. Along the survey, we also draw connections to several related {RL} fields, like hierarchical {RL} and transfer, and other research disciplines, like behavioural psychology. Altogether, the survey presents a broad conceptual overview of planning-learning combinations for {MDP} optimization.},
	journaltitle = {{arXiv}:2006.16712 [cs, stat]},
	author = {Moerland, Thomas M. and Broekens, Joost and Jonker, Catholijn M.},
	urldate = {2021-01-24},
	date = {2020-07-23},
	eprinttype = {arxiv},
	eprint = {2006.16712},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/edoardodavidsanti/Zotero/storage/8D9GVUFQ/Moerland et al. - 2020 - Model-based Reinforcement Learning A Survey.pdf:application/pdf;arXiv.org Snapshot:/Users/edoardodavidsanti/Zotero/storage/9B94NY59/2006.html:text/html},
}

@article{wang_dueling_nodate,
	title = {Dueling Network Architectures for Deep Reinforcement Learning},
	abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, {LSTMs}, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main beneﬁt of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our {RL} agent to outperform the state-of-the-art on the Atari 2600 domain.},
	pages = {9},
	author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and van Hasselt, Hado and Lanctot, Marc and de Freitas, Nando},
	langid = {english},
	file = {Wang et al. - Dueling Network Architectures for Deep Reinforceme.pdf:/Users/edoardodavidsanti/Zotero/storage/24KJV5BW/Wang et al. - Dueling Network Architectures for Deep Reinforceme.pdf:application/pdf},
}

@book{wiskunde_doubleq-learning_nodate,
	title = {{DoubleQ}-learning Hado van Hasselt Multi-agent and Adaptive Computation Group},
	abstract = {In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alternative way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the {optimalpolicyandthatitperformswellinsomesettingsinwhichQ}-learningperformspoorly due toitsoverestimation. 1},
	author = {Wiskunde, Centrum},
	file = {Citeseer - Snapshot:/Users/edoardodavidsanti/Zotero/storage/KXM43JYX/summary.html:text/html;Citeseer - Full Text PDF:/Users/edoardodavidsanti/Zotero/storage/RB8J9TRL/Wiskunde - DoubleQ-learning Hado van Hasselt Multi-agent and .pdf:application/pdf},
}

@incollection{goos_q-learning_1999,
	location = {Berlin, Heidelberg},
	title = {Q-Learning in Continuous State and Action Spaces},
	volume = {1747},
	isbn = {978-3-540-66822-0 978-3-540-46695-6},
	url = {http://link.springer.com/10.1007/3-540-46695-9_35},
	abstract = {Q-learning can be used to learn a control policy that maximises a scalar reward through interaction with the environment. Qlearning is commonly applied to problems with discrete states and actions. We describe a method suitable for control tasks which require continuous actions, in response to continuous states. The system consists of a neural network coupled with a novel interpolator. Simulation results are presented for a non-holonomic control task. Advantage Learning, a variation of Q-learning, is shown enhance learning speed and reliability for this task.},
	pages = {417--428},
	booktitle = {Advanced Topics in Artificial Intelligence},
	publisher = {Springer Berlin Heidelberg},
	author = {Gaskett, Chris and Wettergreen, David and Zelinsky, Alexander},
	editor = {Foo, Norman},
	editorb = {Goos, G. and Hartmanis, J. and van Leeuwen, J.},
	editorbtype = {redactor},
	urldate = {2021-01-24},
	date = {1999},
	langid = {english},
	doi = {10.1007/3-540-46695-9_35},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Gaskett et al. - 1999 - Q-Learning in Continuous State and Action Spaces.pdf:/Users/edoardodavidsanti/Zotero/storage/BRFH3YJ2/Gaskett et al. - 1999 - Q-Learning in Continuous State and Action Spaces.pdf:application/pdf},
}

@article{van_hasselt_deep_2015,
	title = {Deep Reinforcement Learning with Double Q-learning},
	url = {http://arxiv.org/abs/1509.06461},
	abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent {DQN} algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the {DQN} algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
	journaltitle = {{arXiv}:1509.06461 [cs]},
	author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
	urldate = {2021-01-24},
	date = {2015-12-08},
	eprinttype = {arxiv},
	eprint = {1509.06461},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/edoardodavidsanti/Zotero/storage/IY6UAPAG/van Hasselt et al. - 2015 - Deep Reinforcement Learning with Double Q-learning.pdf:application/pdf;arXiv.org Snapshot:/Users/edoardodavidsanti/Zotero/storage/64UMIVT5/1509.html:text/html},
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	pages = {529--533},
	number = {7540},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	urldate = {2021-01-24},
	date = {2015-02},
	langid = {english},
	file = {Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:/Users/edoardodavidsanti/Zotero/storage/9VB8Q5KF/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf},
}

@article{dietterich_hierarchical_1999,
	title = {Hierarchical Reinforcement Learning with the {MAXQ} Value Function Decomposition},
	url = {http://arxiv.org/abs/cs/9905014},
	abstract = {This paper presents the {MAXQ} approach to hierarchical reinforcement learning based on decomposing the target Markov decision process ({MDP}) into a hierarchy of smaller {MDPs} and decomposing the value function of the target {MDP} into an additive combination of the value functions of the smaller {MDPs}. The paper defines the {MAXQ} hierarchy, proves formal results on its representational power, and establishes five conditions for the safe use of state abstractions. The paper presents an online model-free learning algorithm, {MAXQ}-Q, and proves that it converges wih probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the five kinds of state abstraction. The paper evaluates the {MAXQ} representation and {MAXQ}-Q through a series of experiments in three domains and shows experimentally that {MAXQ}-Q (with state abstractions) converges to a recursively optimal policy much faster than flat Q learning. The fact that {MAXQ} learns a representation of the value function has an important benefit: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration. The paper demonstrates the effectiveness of this non-hierarchical execution experimentally. Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoffs in hierarchical reinforcement learning.},
	journaltitle = {{arXiv}:cs/9905014},
	author = {Dietterich, Thomas G.},
	urldate = {2021-01-25},
	date = {1999-05-21},
	eprinttype = {arxiv},
	eprint = {cs/9905014},
	keywords = {Computer Science - Machine Learning, I.2.6},
	file = {arXiv Fulltext PDF:/Users/edoardodavidsanti/Zotero/storage/KA4DB8NE/Dietterich - 1999 - Hierarchical Reinforcement Learning with the MAXQ .pdf:application/pdf;arXiv.org Snapshot:/Users/edoardodavidsanti/Zotero/storage/DETKQAJL/9905014.html:text/html},
}

@inproceedings{simsek_identifying_2005,
	location = {New York, {NY}, {USA}},
	title = {Identifying useful subgoals in reinforcement learning by local graph partitioning},
	isbn = {978-1-59593-180-1},
	url = {https://doi.org/10.1145/1102351.1102454},
	doi = {10.1145/1102351.1102454},
	series = {{ICML} '05},
	abstract = {We present a new subgoal-based method for automatically creating useful skills in reinforcement learning. Our method identifies subgoals by partitioning local state transition graphs---those that are constructed using only the most recent experiences of the agent. The local scope of our subgoal discovery method allows it to successfully identify the type of subgoals we seek---states that lie between two densely-connected regions of the state space while producing an algorithm with low computational cost.},
	pages = {816--823},
	booktitle = {Proceedings of the 22nd international conference on Machine learning},
	publisher = {Association for Computing Machinery},
	author = {Şimşek, Özgür and Wolfe, Alicia P. and Barto, Andrew G.},
	urldate = {2021-01-28},
	date = {2005-08-07},
	file = {Submitted Version:/Users/edoardodavidsanti/Zotero/storage/VMJCWCAV/Şimşek et al. - 2005 - Identifying useful subgoals in reinforcement learn.pdf:application/pdf},
}

@inproceedings{menache_q-cutdynamic_2002,
	location = {Berlin, Heidelberg},
	title = {Q-Cut—Dynamic Discovery of Sub-goals in Reinforcement Learning},
	isbn = {978-3-540-36755-0},
	doi = {10.1007/3-540-36755-1_25},
	series = {Lecture Notes in Computer Science},
	abstract = {We present the Q-Cut algorithm, a graph theoretic approach for automatic detection of sub-goals in a dynamic environment, which is used for acceleration of the Q-Learning algorithm. The learning agent creates an on-line map of the process history, and uses an efficient Max-Flow/Min-Cut algorithm for identifying bottlenecks. The policies for reaching bottlenecks are separately learned and added to the model in a form of options (macro-actions). We then extend the basic Q-Cut algorithm to the Segmented Q-Cut algorithm, which uses previously identified bottlenecks for state space partitioning, necessary for finding additional bottlenecks in complex environments. Experiments show significant performance improvements, particulary in the initial learning phase.},
	pages = {295--306},
	booktitle = {Machine Learning: {ECML} 2002},
	publisher = {Springer},
	author = {Menache, Ishai and Mannor, Shie and Shimkin, Nahum},
	editor = {Elomaa, Tapio and Mannila, Heikki and Toivonen, Hannu},
	date = {2002},
	langid = {english},
	keywords = {Dynamic Discovery, Learning Agent, Markov Decision Process, Reinforcement Learn, Temporal Abstraction},
	file = {Springer Full Text PDF:/Users/edoardodavidsanti/Zotero/storage/IGYGGMJC/Menache et al. - 2002 - Q-Cut—Dynamic Discovery of Sub-goals in Reinforcem.pdf:application/pdf},
}

@article{silver_mastering_2017,
	title = {Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm},
	url = {http://arxiv.org/abs/1712.01815},
	abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the {AlphaGo} Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single {AlphaZero} algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, {AlphaZero} achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
	journaltitle = {{arXiv}:1712.01815 [cs]},
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	urldate = {2021-01-28},
	date = {2017-12-05},
	eprinttype = {arxiv},
	eprint = {1712.01815},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/edoardodavidsanti/Zotero/storage/C47W97DT/Silver et al. - 2017 - Mastering Chess and Shogi by Self-Play with a Gene.pdf:application/pdf;arXiv.org Snapshot:/Users/edoardodavidsanti/Zotero/storage/NDHSTHXC/1712.html:text/html},
}

@article{sutton_dyna_1991,
	title = {Dyna, an integrated architecture for learning, planning, and reacting},
	volume = {2},
	issn = {0163-5719},
	url = {https://doi.org/10.1145/122344.122377},
	doi = {10.1145/122344.122377},
	abstract = {Dyna is an {AI} architecture that integrates learning, planning, and reactive execution. Learning methods are used in Dyna both for compiling planning results and for updating a model of the effects of the agent's actions on the world. Planning is incremental and can use the probabilistic and ofttimes incorrect world models generated by learning processes. Execution is fully reactive in the sense that no planning intervenes between perception and action. Dyna relies on machine learning methods for learning from examples---these are among the basic building blocks making up the architecture---yet is not tied to any particular method. This paper briefly introduces Dyna and discusses its strengths and weaknesses with respect to other architectures.},
	pages = {160--163},
	number = {4},
	journaltitle = {{ACM} {SIGART} Bulletin},
	shortjournal = {{SIGART} Bull.},
	author = {Sutton, Richard S.},
	urldate = {2021-01-28},
	date = {1991-07-01},
	file = {Submitted Version:/Users/edoardodavidsanti/Zotero/storage/Q9FDPYIS/Sutton - 1991 - Dyna, an integrated architecture for learning, pla.pdf:application/pdf},
}

@article{bellman_dynamic_1966,
	title = {Dynamic Programming},
	volume = {153},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.153.3731.34},
	doi = {10.1126/science.153.3731.34},
	pages = {34--37},
	number = {3731},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Bellman, R.},
	urldate = {2021-01-28},
	date = {1966-07-01},
	langid = {english},
	file = {Bellman - 1966 - Dynamic Programming.pdf:/Users/edoardodavidsanti/Zotero/storage/RQ7ZK7SM/Bellman - 1966 - Dynamic Programming.pdf:application/pdf},
}

@article{watkins_q-learning_1992,
	title = {Q-learning},
	volume = {8},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00992698},
	doi = {10.1007/BF00992698},
	abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
	pages = {279--292},
	number = {3},
	journaltitle = {Machine Learning},
	shortjournal = {Mach Learn},
	author = {Watkins, Christopher J. C. H. and Dayan, Peter},
	urldate = {2021-01-29},
	date = {1992-05-01},
	langid = {english},
	file = {Springer Full Text PDF:/Users/edoardodavidsanti/Zotero/storage/2X6VE4PM/Watkins and Dayan - 1992 - Q-learning.pdf:application/pdf},
}

@article{sutton_reinforcement_nodate,
	title = {Reinforcement Learning: An Introduction},
	pages = {352},
	author = {Sutton, Richard S and Barto, Andrew G},
	langid = {english},
	file = {Sutton and Barto - Reinforcement Learning An Introduction.pdf:/Users/edoardodavidsanti/Zotero/storage/QHQZLKAC/Sutton and Barto - Reinforcement Learning An Introduction.pdf:application/pdf},
}

@article{silver_mastering_2017-1,
	title = {Mastering the game of Go without human knowledge},
	volume = {550},
	rights = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature24270},
	doi = {10.1038/nature24270},
	abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, {AlphaGo} became the first program to defeat a world champion in the game of Go. The tree search in {AlphaGo} evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. {AlphaGo} becomes its own teacher: a neural network is trained to predict {AlphaGo}’s own move selections and also the winner of {AlphaGo}’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program {AlphaGo} Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating {AlphaGo}.},
	pages = {354--359},
	number = {7676},
	journaltitle = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	urldate = {2021-01-29},
	date = {2017-10},
	langid = {english},
	note = {Number: 7676
Publisher: Nature Publishing Group},
	file = {Full Text PDF:/Users/edoardodavidsanti/Zotero/storage/DMGAX4TT/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:application/pdf;Snapshot:/Users/edoardodavidsanti/Zotero/storage/32WADMP9/nature24270.html:text/html},
}

@article{yu_reinforcement_2020,
	title = {Reinforcement Learning in Healthcare: A Survey},
	url = {http://arxiv.org/abs/1908.08796},
	shorttitle = {Reinforcement Learning in Healthcare},
	abstract = {As a subfield of machine learning, reinforcement learning ({RL}) aims at empowering one's capabilities in behavioural decision making by using interaction experience with the world and an evaluative feedback. Unlike traditional supervised learning methods that usually rely on one-shot, exhaustive and supervised reward signals, {RL} tackles with sequential decision making problems with sampled, evaluative and delayed feedback simultaneously. Such distinctive features make {RL} technique a suitable candidate for developing powerful solutions in a variety of healthcare domains, where diagnosing decisions or treatment regimes are usually characterized by a prolonged and sequential procedure. This survey discusses the broad applications of {RL} techniques in healthcare domains, in order to provide the research community with systematic understanding of theoretical foundations, enabling methods and techniques, existing challenges, and new insights of this emerging paradigm. By first briefly examining theoretical foundations and key techniques in {RL} research from efficient and representational directions, we then provide an overview of {RL} applications in healthcare domains ranging from dynamic treatment regimes in chronic diseases and critical care, automated medical diagnosis from both unstructured and structured clinical data, as well as many other control or scheduling domains that have infiltrated many aspects of a healthcare system. Finally, we summarize the challenges and open issues in current research, and point out some potential solutions and directions for future research.},
	journaltitle = {{arXiv}:1908.08796 [cs]},
	author = {Yu, Chao and Liu, Jiming and Nemati, Shamim},
	urldate = {2021-01-29},
	date = {2020-04-24},
	eprinttype = {arxiv},
	eprint = {1908.08796},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/edoardodavidsanti/Zotero/storage/PERI9TYD/Yu et al. - 2020 - Reinforcement Learning in Healthcare A Survey.pdf:application/pdf;arXiv.org Snapshot:/Users/edoardodavidsanti/Zotero/storage/WBKNSGRE/1908.html:text/html},
}

@online{noauthor_google_nodate,
	title = {Google {AI} Blog: Scalable Deep Reinforcement Learning for Robotic Manipulation},
	url = {https://ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html},
	urldate = {2021-01-29},
}

@article{kiran_deep_2021,
	title = {Deep Reinforcement Learning for Autonomous Driving: A Survey},
	url = {http://arxiv.org/abs/2002.00444},
	shorttitle = {Deep Reinforcement Learning for Autonomous Driving},
	abstract = {With the development of deep representation learning, the domain of reinforcement learning ({RL}) has become a powerful learning framework now capable of learning complex policies in high dimensional environments. This review summarises deep reinforcement learning ({DRL}) algorithms and provides a taxonomy of automated driving tasks where (D){RL} methods have been employed, while addressing key computational challenges in real world deployment of autonomous driving agents. It also delineates adjacent domains such as behavior cloning, imitation learning, inverse reinforcement learning that are related but are not classical {RL} algorithms. The role of simulators in training agents, methods to validate, test and robustify existing solutions in {RL} are discussed.},
	journaltitle = {{arXiv}:2002.00444 [cs]},
	author = {Kiran, B. Ravi and Sobh, Ibrahim and Talpaert, Victor and Mannion, Patrick and Sallab, Ahmad A. Al and Yogamani, Senthil and Pérez, Patrick},
	urldate = {2021-01-29},
	date = {2021-01-23},
	eprinttype = {arxiv},
	eprint = {2002.00444},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/edoardodavidsanti/Zotero/storage/BIF4ZMMH/Kiran et al. - 2021 - Deep Reinforcement Learning for Autonomous Driving.pdf:application/pdf;arXiv.org Snapshot:/Users/edoardodavidsanti/Zotero/storage/ACQTJDJ3/2002.html:text/html},
}

@article{paulus_deep_2017,
	title = {A Deep Reinforced Model for Abstractive Summarization},
	url = {https://arxiv.org/abs/1705.04304v3},
	abstract = {Attentional, {RNN}-based encoder-decoder models for abstractive summarization
have achieved good performance on short input and output sequences. For longer
documents and summaries however these models often include repetitive and
incoherent phrases. We introduce a neural network model with a novel
intra-attention that attends over the input and continuously generated output
separately, and a new training method that combines standard supervised word
prediction and reinforcement learning ({RL}). Models trained only with supervised
learning often exhibit "exposure bias" - they assume ground truth is provided
at each step during training. However, when standard word prediction is
combined with the global sequence prediction training of {RL} the resulting
summaries become more readable. We evaluate this model on the {CNN}/Daily Mail
and New York Times datasets. Our model obtains a 41.16 {ROUGE}-1 score on the
{CNN}/Daily Mail dataset, an improvement over previous state-of-the-art models.
Human evaluation also shows that our model produces higher quality summaries.},
	author = {Paulus, Romain and Xiong, Caiming and Socher, Richard},
	urldate = {2021-01-29},
	date = {2017-05-11},
	langid = {english},
	file = {Full Text PDF:/Users/edoardodavidsanti/Zotero/storage/T86HZHGI/Paulus et al. - 2017 - A Deep Reinforced Model for Abstractive Summarizat.pdf:application/pdf},
}

@article{zhang_dq_2018,
	title = {{DQ} Scheduler: Deep Reinforcement Learning Based Controller Synchronization in Distributed {SDN}},
	url = {http://arxiv.org/abs/1812.00852},
	shorttitle = {{DQ} Scheduler},
	abstract = {In distributed software-defined networks ({SDN}), multiple physical {SDN} controllers, each managing a network domain, are implemented to balance centralized control, scalability and reliability requirements. In such networking paradigm, controllers synchronize with each other to maintain a logically centralized network view. Despite various proposals of distributed {SDN} controller architectures, most existing works only assume that such logically centralized network view can be achieved with some synchronization designs, but the question of how exactly controllers should synchronize with each other to maximize the benefits of synchronization under the eventual consistency assumptions is largely overlooked. To this end, we formulate the controller synchronization problem as a Markov Decision Process ({MDP}) and apply reinforcement learning techniques combined with deep neural network to train a smart controller synchronization policy, which we call the Deep-Q ({DQ}) Scheduler. Evaluation results show that {DQ} Scheduler outperforms the antientropy algorithm implemented in the {ONOS} controller by up to 95.2\% for inter-domain routing tasks.},
	journaltitle = {{arXiv}:1812.00852 [cs]},
	author = {Zhang, Ziyao and Ma, Liang and Poularakis, Konstantinos and Leung, Kin K. and Wu, Lingfei},
	urldate = {2021-01-29},
	date = {2018-12-03},
	eprinttype = {arxiv},
	eprint = {1812.00852},
	keywords = {Computer Science - Networking and Internet Architecture},
	file = {arXiv Fulltext PDF:/Users/edoardodavidsanti/Zotero/storage/VQ6TM6L9/Zhang et al. - 2018 - DQ Scheduler Deep Reinforcement Learning Based Co.pdf:application/pdf;arXiv.org Snapshot:/Users/edoardodavidsanti/Zotero/storage/8JEX5KPV/1812.html:text/html},
}

@inproceedings{zhang_q-placement_2018,
	title = {Q-Placement: Reinforcement-Learning-Based Service Placement in Software-Defined Networks},
	doi = {10.1109/ICDCS.2018.00159},
	shorttitle = {Q-Placement},
	abstract = {In software-defined networking ({SDN}) paradigm, where the control and data plane are separated, the scalability of the {SDN} controller in the control plane is critical and can affect the overall network performance significantly. To improve controller scalability, efforts have been put into enhancing the capability of {SDN} switches in the data plane, to make them more autonomous in providing routine services without consulting the controller. In this regard, we investigate the service placement problem on {SDN} switches aiming at minimizing the average accumulated service costs for end users. To solve this problem, we propose a novel reinforcement-learning-based algorithm with guaranteed performance and convergence rate, called Q-placement. Comparing to traditional optimization techniques, Q-placement exhibits many appealing features, such as performance-tuneable optimization and off-the-shelf implementation. Extensive evaluations show that Q-placement consistently outperforms benchmarks and other state-of-the-art algorithms in both synthetic and real networks. Moreover, these evaluations reveal insights into how the network topological properties (e.g., density), servicing capacities, and controller's roles affect the accumulated service costs, which is useful in service planning tasks.},
	eventtitle = {2018 {IEEE} 38th International Conference on Distributed Computing Systems ({ICDCS})},
	pages = {1527--1532},
	booktitle = {2018 {IEEE} 38th International Conference on Distributed Computing Systems ({ICDCS})},
	author = {Zhang, Z. and Ma, L. and Leung, K. K. and Tassiulas, L. and Tucker, J.},
	date = {2018-07},
	note = {{ISSN}: 2575-8411},
	keywords = {average accumulated service costs, control plane, Control systems, controller scalability, convergence rate, data plane, Decision making, Electronic mail, learning (artificial intelligence), network performance, network topological properties, Network topology, optimisation, Optimization, optimization techniques, performance-tuneable optimization, q learning, Q-placement, real networks, reinforcement learning, reinforcement-learning-based algorithm, reinforcement-learning-based service placement, Scalability, {SDN}, {SDN} controller, service placement, service placement problem, service planning tasks, software defined networking, software-defined networking paradigm, synthetic networks, Task analysis, telecommunication network topology},
	file = {IEEE Xplore Abstract Record:/Users/edoardodavidsanti/Zotero/storage/U84GPTUF/8416422.html:text/html},
}

@article{watkins_learning_1989,
	title = {Learning From Delayed Rewards},
	abstract = {Photocopy. Supplied by British Library. Thesis (Ph. D.)--King's College, Cambridge, 1989.},
	author = {Watkins, Christopher},
	date = {1989-01-01},
	file = {Full Text PDF:/Users/edoardodavidsanti/Zotero/storage/TVX56IK9/Watkins - 1989 - Learning From Delayed Rewards.pdf:application/pdf},
}

@article{parr_reinforcement_nodate,
	title = {Reinforcement Learning with Hierarchies of Machines},
	abstract = {We present a new approach to reinforcement learning in which the policies considered by the learning process are constrained by hierarchies of partially speciﬁed machines. This allows for the use of prior knowledge to reduce the search space and provides a framework in which knowledge can be transferred across problems and in which component solutions can be recombined to solve larger and more complicated problems. Our approach can be seen as providing a link between reinforcement learning and “behavior-based” or “teleo-reactive” approaches to control. We present provably convergent algorithms for problem-solving and learning with hierarchical machines and demonstrate their effectiveness on a problem with several thousand states.},
	pages = {7},
	author = {Parr, Ronald and Russell, Stuart},
	langid = {english},
	file = {Parr and Russell - Reinforcement Learning with Hierarchies of Machine.pdf:/Users/edoardodavidsanti/Zotero/storage/CQ9KD3T3/Parr and Russell - Reinforcement Learning with Hierarchies of Machine.pdf:application/pdf},
}

@article{sutton_between_1999,
	title = {Between {MDPs} and semi-{MDPs}: A framework for temporal abstraction in reinforcement learning},
	volume = {112},
	issn = {0004-3702},
	url = {http://www.sciencedirect.com/science/article/pii/S0004370299000521},
	doi = {10.1016/S0004-3702(99)00052-1},
	shorttitle = {Between {MDPs} and semi-{MDPs}},
	abstract = {Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for {AI}. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes ({MDPs}). We extend the usual notion of action in this framework to include options—closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options defined over an {MDP} constitutes a semi-Markov decision process ({SMDP}), and the theory of {SMDPs} provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying {MDP} and the {SMDP} and are thus beyond {SMDP} theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro-utility problem.},
	pages = {181--211},
	number = {1},
	journaltitle = {Artificial Intelligence},
	shortjournal = {Artificial Intelligence},
	author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder},
	urldate = {2021-01-29},
	date = {1999-08-01},
	langid = {english},
	keywords = {Hierarchical planning, Intra-option learning, Macroactions, Macros, Markov decision processes, Options, Reinforcement learning, Semi-Markov decision processes, Subgoals, Temporal abstraction},
	file = {ScienceDirect Full Text PDF:/Users/edoardodavidsanti/Zotero/storage/YQAWYF9U/Sutton et al. - 1999 - Between MDPs and semi-MDPs A framework for tempor.pdf:application/pdf;ScienceDirect Snapshot:/Users/edoardodavidsanti/Zotero/storage/JPI62I5P/S0004370299000521.html:text/html},
}

@inreference{noauthor_markov_2021,
	title = {Markov decision process},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Markov_decision_process&oldid=1002665350},
	abstract = {In mathematics, a Markov decision process ({MDP}) is a discrete-time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. {MDPs} are useful for studying optimization problems solved via dynamic programming. {MDPs} were known at least as early as the 1950s; a core body of research on Markov decision processes resulted from Ronald Howard's 1960 book, Dynamic Programming and Markov Processes. They are used in many disciplines, including robotics, automatic control, economics and manufacturing. The name of {MDPs} comes from the Russian mathematician Andrey Markov as they are an extension of Markov chains.
At each time step, the process is in some state 
  
    
      
        s
      
    
    \{{\textbackslash}displaystyle s\}
  , and the decision maker may choose any action 
  
    
      
        a
      
    
    \{{\textbackslash}displaystyle a\}
   that is available in state 
  
    
      
        s
      
    
    \{{\textbackslash}displaystyle s\}
  . The process responds at the next time step by randomly moving into a new state 
  
    
      
        
          s
          ′
        
      
    
    \{{\textbackslash}displaystyle s'\}
  , and giving the decision maker a corresponding reward 
  
    
      
        
          R
          
            a
          
        
        (
        s
        ,
        
          s
          ′
        
        )
      
    
    \{{\textbackslash}displaystyle R\_\{a\}(s,s')\}
  .
The probability that the process moves into its new state 
  
    
      
        
          s
          ′
        
      
    
    \{{\textbackslash}displaystyle s'\}
   is influenced by the chosen action. Specifically, it is given by the state transition function 
  
    
      
        
          P
          
            a
          
        
        (
        s
        ,
        
          s
          ′
        
        )
      
    
    \{{\textbackslash}displaystyle P\_\{a\}(s,s')\}
  . Thus, the next state 
  
    
      
        
          s
          ′
        
      
    
    \{{\textbackslash}displaystyle s'\}
   depends on the current state 
  
    
      
        s
      
    
    \{{\textbackslash}displaystyle s\}
   and the decision maker's action 
  
    
      
        a
      
    
    \{{\textbackslash}displaystyle a\}
  . But given 
  
    
      
        s
      
    
    \{{\textbackslash}displaystyle s\}
   and 
  
    
      
        a
      
    
    \{{\textbackslash}displaystyle a\}
  , it is conditionally independent of all previous states and actions; in other words, the state transitions of an {MDP} satisfy the Markov property.
Markov decision processes are an extension of Markov chains; the difference is the addition of actions (allowing choice) and rewards (giving motivation). Conversely, if only one action exists for each state (e.g. "wait") and all rewards are the same (e.g. "zero"), a Markov decision process reduces to a Markov chain.},
	booktitle = {Wikipedia},
	urldate = {2021-01-29},
	date = {2021-01-25},
	langid = {english},
	note = {Page Version {ID}: 1002665350},
	file = {Snapshot:/Users/edoardodavidsanti/Zotero/storage/4ZXCTJDM/index.html:text/html},
}

@video{noauthor_openai_nodate,
	title = {{OpenAI} Gym: the {CartPole}-v0 environment},
	url = {https://gym.openai.com/envs/CartPole-v0},
	shorttitle = {{OpenAI} Gym},
	abstract = {See the scores on all {CartPole}-v0 evaluations.},
	urldate = {2021-01-29},
	langid = {english},
	file = {Snapshot:/Users/edoardodavidsanti/Zotero/storage/5EGCCQNX/CartPole-v0.html:text/html},
}

@online{noauthor_cloud_nodate,
	title = {Cloud Tensor Processing Units ({TPUs})},
	url = {https://cloud.google.com/tpu/docs/tpus},
	titleaddon = {Google Cloud},
	urldate = {2021-01-29},
	langid = {english},
	file = {Snapshot:/Users/edoardodavidsanti/Zotero/storage/DE6267AU/tpus.html:text/html},
}

@article{barto_recent_2003,
	title = {Recent Advances in Hierarchical Reinforcement Learning},
	volume = {13},
	issn = {1573-7594},
	url = {https://doi.org/10.1023/A:1022140919877},
	doi = {10.1023/A:1022140919877},
	abstract = {Reinforcement learning is bedeviled by the curse of dimensionality: the number of parameters to be learned grows exponentially with the size of any compact encoding of a state. Recent attempts to combat the curse of dimensionality have turned to principled ways of exploiting temporal abstraction, where decisions are not required at each step, but rather invoke the execution of temporally-extended activities which follow their own policies until termination. This leads naturally to hierarchical control architectures and associated learning algorithms. We review several approaches to temporal abstraction and hierarchical organization that machine learning researchers have recently developed. Common to these approaches is a reliance on the theory of semi-Markov decision processes, which we emphasize in our review. We then discuss extensions of these ideas to concurrent activities, multiagent coordination, and hierarchical memory for addressing partial observability. Concluding remarks address open challenges facing the further development of reinforcement learning in a hierarchical setting.},
	pages = {41--77},
	number = {1},
	journaltitle = {Discrete Event Dynamic Systems},
	shortjournal = {Discrete Event Dynamic Systems},
	author = {Barto, Andrew G. and Mahadevan, Sridhar},
	urldate = {2021-02-01},
	date = {2003-01-01},
	langid = {english},
	file = {Springer Full Text PDF:/Users/edoardodavidsanti/Zotero/storage/LK52BP28/Barto and Mahadevan - 2003 - Recent Advances in Hierarchical Reinforcement Lear.pdf:application/pdf},
}

@online{noauthor_tensorflow_nodate,
	title = {{TensorFlow}},
	url = {https://www.tensorflow.org/},
	urldate = {2021-02-01},
	file = {TensorFlow:/Users/edoardodavidsanti/Zotero/storage/6QWCQTNY/www.tensorflow.org.html:text/html},
}

@online{openai_gym_nodate,
	title = {Gym: A toolkit for developing and comparing reinforcement learning algorithms},
	url = {https://gym.openai.com},
	shorttitle = {Gym},
	abstract = {A toolkit for developing and comparing reinforcement learning algorithms},
	author = {{OpenAI}},
	urldate = {2021-02-01},
	file = {Snapshot:/Users/edoardodavidsanti/Zotero/storage/S8ICQT5P/gym.openai.com.html:text/html},
}

@article{kingma_adam_2017,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
	journaltitle = {{arXiv}:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2021-05-31},
	date = {2017-01-29},
	eprinttype = {arxiv},
	eprint = {1412.6980},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/edoardodavidsanti/Zotero/storage/SY5QXCYT/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/edoardodavidsanti/Zotero/storage/2C698MCM/1412.html:text/html},
}

@article{rusu_progressive_2016,
	title = {Progressive Neural Networks},
	url = {http://arxiv.org/abs/1606.04671},
	abstract = {Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
	journaltitle = {{arXiv}:1606.04671 [cs]},
	author = {Rusu, Andrei A. and Rabinowitz, Neil C. and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
	urldate = {2021-06-03},
	date = {2016-09-07},
	eprinttype = {arxiv},
	eprint = {1606.04671},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/edoardodavidsanti/Zotero/storage/L2J4SFP7/Rusu et al. - 2016 - Progressive Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/edoardodavidsanti/Zotero/storage/7U9MGGGK/1606.html:text/html},
}

@online{sharma_what_2019,
	title = {What the Hell is Perceptron?},
	url = {https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53},
	abstract = {The Fundamentals of Neural Networks},
	titleaddon = {Medium},
	author = {{SHARMA}, {SAGAR}},
	urldate = {2021-06-11},
	date = {2019-10-11},
	langid = {english},
	file = {Snapshot:/Users/edoardodavidsanti/Zotero/storage/I463TJTK/what-the-hell-is-perceptron-626217814f53.html:text/html},
}