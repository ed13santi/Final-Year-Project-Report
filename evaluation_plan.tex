\chapter{Evaluation Plan}

%Differently from many other projects, this project does not have as a goal a certain application for which different techniques
%can be used. Thus the eventual success of the project can't be evaluated by the success of the application. In this project, we
%are developing the technique itself, which could then be applied in different contexts. Since the project consists of the
%development of a new algorithm, there is no certainty that the algorithm would achieve good performance. Thus, the project can be
%considered successful even if the state-decomposition algorithm does not show improved performance.

The main objective of this project is to evaluate the performance of the proposed state-decomposition algorithm. To do so, the
algorithm will be tested in different varieties in the TaxiTraps environment described in Section-\ref{sec:TaxiTraps}. The
possible varieties are:

\begin{itemize}
    \item In the original state-decomposition algorithm it is assumed that the NNs of the sub-spaces' agents keep training after
    they are joined together in the second stage of the training as they are part of the bigger newly formed neural network of the
    global agent. Alternatively, we could freeze the weights of the sub-spaces' NNs (trained in the 1st stage) during the second
    stage of training. 
    
    \item Once the NNs are joined in the 2nd stage, the training could be done using all transitions, only those that start and
    end in different sub-space\footnote{To take into account the transitions between different sub-spaces.} or start with only
    transitions between different sub-spaces\footnote{At this stage of training we have many of these transitions that were saved
    but ignored in the first stage of training.} and then as the global agent experiences more transitions use all possible
    transitions.
    
    \item The time at which the agents of the sub-spaces should be joined together to form the global agent is not defined.
    Different strategies can be compared, such as waiting for the sub-agents' action-values to converge or defining different
    minimum rates of change of the average episode rewards going below which would start the second stage of training.
\end{itemize}

\paragraph{}
The main aspect of these algorithms that will be evaluated is the sample efficiency, which is how efficiently the algorithm uses
the samples that it experiences to learn a policy. This affects how quickly the algorithm can learn an optimal policy and it can
be measured by plotting the total reward per episode vs the number of training steps. It is also important to consider the fact
that the learning process is stochastic and the performance varies between trials, hence we normally express the reward per
episode using a confidence bound.

\begin{figure}[H]
%\begin{wrapfigure}{r}{0.5\linewidth}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/exampleOfGraph.PNG}
    \caption{Example of reward per episode vs number of training steps. Note the confidence bands to express the uncertainty in
    the results. Figure taken from \cite{stateActionEmbeddings}}
    \label{fig:my_label}
%\end{wrapfigure}
\end{figure}

\paragraph{}
All these variations of the state-decomposition method will be compared with a standard DQN agent. There are many scenarios where
the system is known to be formed by almost independent sub-systems corresponding to almost independent sub-problems. In such a
scenario, the states could be grouped into separate sub-spaces without even estimating the state-transition matrix by just
leveraging the knowledge of the characteristics of the system. Thus, we shall compare the performance of the state-decomposition
method when the state-transition matrix is given (or sub-spaces can be guessed) at the start and when it has to be estimated from
samples. In the latter case, we can estimate the model of the environment is we also consider the rewards of the samples. Thus we
shall also compare the performance of the state-decomposition algorithm with a method that simply approximates the model of the
environment from samples and then finds the optimal policy using planning algorithms. 

THINGS TO CARE ABOUT:

When I use the deltaswitch, how the weights updated for NN0 when NN1 is active and vice-versa.
https://www.gatsby.ucl.ac.uk/~dayan/papers/cjch.pdf https://arxiv.org/pdf/1901.00137.pdf relationship with drpout gains might be
due to effects of double q-learning rather than decomposition resource allocation / job allocation alibaba datasetF


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
