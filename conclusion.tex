\chapter{Conclusions and Further Work}

- ONE POSSIBLE LIMITATION IS THAT I TRIED ALL THIS STUFF IN RELATIVELY SIMPLE SCENARIOS, WHERE THE NETWROSK DOES NOT NECESSARILY
NEED TO GENERALISE. I THINK THAT IN THE CASE OF LIKE A VIDEO-GAME OR SOMETHING LIKE THAT, THE STATE-DECOMPOSITION WOULD BE EVEN
MORE HELPFUL, AS IT WOULD MEAN THAT THE NETWORKS HAVE TO GENERALISE TO A SMALLER SUB-SPACE THAN THE ORIGINAL STATE-SPACE.

- could try to use state-decomposition with policy algorithms

- actually I think that what makes a difference is more like a better decoding

-i f you think about it isn't deltaswitch just a type of hierarchical learning? What's the difference?

-DeltaSwitch is basically a hierarchical agent that works for environemnts that are not perfectly decomposable too

- Need to explain that this project focussed on decomposition using tranisiotn probabilities, however this is not the only kind
and actullay the example given in the data centre part might give an even better decomposition.

- other limitations of my work, small number of samples and incomplete range of scenarios to be tested due to time constraints and
computational constraints

The time at which the agents of the sub-spaces should be joined together to form the global agent is not defined.
    Different strategies can be compared, such as waiting for the sub-agents' action-values to converge or defining different
    minimum rates of change of the average episode rewards going below which would start the second stage of training.


Disadvantages of state-decomposition: need to determine decomposition, more complicated system, not guaranteed to be faster


\section{Extensions}

\paragraph{Compression and embeddings of states} The state-decomposition algorithm divides the original state-space of the system
into smaller groups of states. It can be supposed that the variance of the states within these sub-spaces is smaller than in the
original state space. Thus, the states that are input in the sub-spaces' agents could be successfully compressed into a more
compact representation which could positively affect the performance of the algorithm. More generally, it could be beneficial to
learn custom embeddings for the states of each sub-space, having the aim of better generalisation rather than state-space
reduction.

\paragraph{Applying state-decomposition to continuous state-spaces} The state-decomposition method is only applicable to discrete
state spaces. A possible extension would investigate methods to decompose continuous state-spaces. A state-transition matrix could
be obtained by discretising the states and the state-space would be decomposed accordingly, while the agents could still be fed
the original continuous states as input.

\paragraph{Applying threshold to transitions between whole sub-spaces} The original state-decomposition method in this project
consists of temporarily ignoring improbable state transitions by applying a threshold to the state-transition matrix. There are
scenarios in which groups of states are connected by state-transitions that have a high probability but it would still be useful
to consider them as separate sub-spaces. Consider for example Figure-\ref{fig:connected-states}, where an MDP is composed of
nearly independent sub-spaces that are only connected by the blue transitions. In such a scenario it could be useful to determine
the sub-spaces by limiting the number of possible non-zero transition probabilities between any two sub-spaces rather than
applying a probability threshold to every single transition. Alternatively, we could apply a threshold to transition probabilities
between sub-spaces instead of single states.

\paragraph{Offline training} The samples obtained from the pre-training interaction required to obtain the state-transition matrix
could be saved and later added to the experience replay memories of the DQN agents. Offline pre-training could also be conducted
using these samples as this could improve sample efficiency. Similarly, transition samples between different sub-spaces are not
used in the first stage of training. These could be used to conduct offline pre-training of the merged network.

\paragraph{Sharing information between sub-agents} YE basically the link between this and hierarchical innit

\input{graphs/connected-states}

\paragraph{Using state-decomposition for automatic sub-goal discovery} The idea of state-decomposition, especially when
implemented by considering transitions between whole sub-spaces could be applied to the automatic discovery of sub-goals in
hierarchical reinforcement learning. A similar idea has been adopted by \cite{simsek_identifying_2005} and
\cite{menache_q-cutdynamic_2002}, in which the graph of the MDP is divided into sub-spaces by cutting through the least possible
number of state-transitions. The intuition is that sub-goals, such as driving towards the passenger in "Taxi-v3" or driving to the
destination after picking it up, are often connected by bottleneck states, such as states 3, 6, 9 and 12 in
Figure-\ref{fig:connected-states}.


%\paragraph{No NN version} For the ersion without neural networks, the final neural netowrk could be attached earlier than
%achiving convergence if we still allow to update the intermediate q-values during training

%if I do without NN I don't think it's really state decomposition. It's more like I initially do q-learning in a simple
%environment that disregards interactions between subsystems and then I append a NN at the end to take into account these
%interatctions, Seems like a very weird thing to do doesn't it

%\paragraph{aybe prove some mathematical bounds}

%\paragraph{Possible expansion of looking at how policy affects the decomposition}

\subsection[Applications]{Application to real-life scenarios}

Resource allocation problem, 2 data centres, allocate job to 2nd data-centre
Driving taxi
Alibaba thing