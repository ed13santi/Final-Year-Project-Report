\chapter{Conclusions and Further Work}
How successful have you been? What have you
achieved? How could the work be taken further given
more time (perhaps by another student next year)? It
is important here to identify positively what is
worthwhile in your work. At the same time, honesty,
and a clear description of the limits of your work, is
equally important. It is often most appropriate to
describe work you did not have time to complete as
further work.
Your readers will not be clear where, in your long
report, are your most significant achievements. In the
conclusions you must summarise this, referring as
necessary to other sections for more detail.
- What design choices did you have along the
way, and why did you make the choices you
made?
- What was the most difficult and/or clever part
of the project?
- Why was it difficult?
- How did you overcome the difficulties?
- Did you discover or invent anything novel?
- What did you learn?
Note that “difficult” does not necessarily mean the
thing that took you the longest amount of time. Note
also that the conclusions must concisely summarise
this material, and refer to other sections for the
details.

\section{Extensions}

\paragraph{Compression and embeddings of states} The state-decomposition algorithm divides the original state-space of the system
into smaller groups of states. It can be supposed that the variance of the states within these sub-spaces is smaller than in the
original state space. Thus, the states that are input in the sub-spaces' agents could be successfully compressed into a more
compact representation which could positively affect the performance of the algorithm. More generally, it could be beneficial to
learn custom embeddings for the states of each sub-space, having the aim of better generalisation rather than state-space
reduction.

\paragraph{Applying state-decomposition to continuous state-spaces} The state-decomposition method is only applicable to discrete
state spaces. A possible extension would investigate methods to decompose continuous state-spaces. A state-transition matrix could
be obtained by discretising the states and the state-space would be decomposed accordingly, while the agents could still be fed
the original continuous states as input.

\paragraph{Applying threshold to transitions between whole sub-spaces} The original state-decomposition method in this project
consists of temporarily ignoring improbable state transitions by applying a threshold to the state-transition matrix. There are
scenarios in which groups of states are connected by state-transitions that have a high probability but it would still be useful
to consider them as separate sub-spaces. Consider for example Figure-\ref{fig:connected-states}, where an MDP is composed of
nearly independent sub-spaces that are only connected by the blue transitions. In such a scenario it could be useful to determine
the sub-spaces by limiting the number of possible non-zero transition probabilities between any two sub-spaces rather than
applying a probability threshold to every single transition. Alternatively, we could apply a threshold to transition probabilities
between sub-spaces instead of single states.

\paragraph{Offline training} The samples obtained from the pre-training interaction required to obtain the state-transition matrix
could be saved and later added to the experience replay memories of the DQN agents. Offline pre-training could also be conducted
using these samples as this could improve sample efficiency. Similarly, transition samples between different sub-spaces are not
used in the first stage of training. These could be used to conduct offline pre-training of the merged network.

%\paragraph{Sharing information between sub-agents}

\input{graphs/connected-states}

\paragraph{Using state-decomposition for automatic sub-goal discovery} The idea of state-decomposition, especially when
implemented by considering transitions between whole sub-spaces could be applied to the automatic discovery of sub-goals in
hierarchical reinforcement learning. A similar idea has been adopted by \cite{simsek_identifying_2005} and
\cite{menache_q-cutdynamic_2002}, in which the graph of the MDP is divided into sub-spaces by cutting through the least possible
number of state-transitions. The intuition is that sub-goals, such as driving towards the passenger in "Taxi-v3" or driving to the
destination after picking it up, are often connected by bottleneck states, such as states 3, 6, 9 and 12 in
Figure-\ref{fig:connected-states}.


%\paragraph{No NN version} For the ersion without neural networks, the final neural netowrk could be attached earlier than
%achiving convergence if we still allow to update the intermediate q-values during training

%if I do without NN I don't think it's really state decomposition. It's more like I initially do q-learning in a simple
%environment that disregards interactions between subsystems and then I append a NN at the end to take into account these
%interatctions, Seems like a very weird thing to do doesn't it

%\paragraph{aybe prove some mathematical bounds}

%\paragraph{Possible expansion of looking at how policy affects the decomposition}

\section[Applications]{Application to real-life scenarios}

Resource allocation problem, 2 data centres, allocate job to 2nd data-centre
Driving taxi
Alibaba thing