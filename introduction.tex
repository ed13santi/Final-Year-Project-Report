\chapter{Introduction}

\section{Motivation}

%The online nature of this family of algorithms causes them to focus on learning optimal decisions in the most visited states at
%the expense of the least visited ones. $

\paragraph{}
Reinforcement learning (RL) has been a very active research area in the last few years, achieving impressive feats in a wide range
of applications. Some advantages of reinforcement learning algorithms are great flexibility, the capability of solving both very
low and high-level problems \cite{sutton_barto_2018} and not requiring a model of the environment in which they are used. RL has
famously been applied to games, such DeepMind's AlphaGo \cite{silver_mastering_2017}, a program that in 2016 beat the
top-ranked Go player in the world. Other successful applications include healthcare \cite{yu_reinforcement_2020}, robotics
\cite{noauthor_google_nodate}, autonomous driving \cite{autonomousDriving} and natural language processing \cite{NLP}. My project
supervisor Prof. K. K. Leung has also been working on applications of RL in software-defined networks (SDNs), such as for the
control of the placement of services \cite{ServicePlacement} and the synchronisation of controllers in distributed SDNs
\cite{SDNSynchronisation}.

\paragraph{}
Dealing with very large systems is one of the main problems encountered in the application of reinforcement learning in practical
environments. RL is based on learning through interaction with the environment. As the size of the system increases, so does the
complexity of the problem and it becomes increasingly difficult to learn how to interact with the environment, often making the
learning process excessively long. In practical scenarios, it might not be feasible to train an RL agent for long enough for it to
be a feasible solution, as this might incur excessive costs or, in the case that the behaviour of the environment changes over
time, the algorithm might not be able to learn fast enough to adapt to the changing environment.

\section{Project Definition}

\paragraph{}
The goal of this project is to develop a state-decomposition method, that aims to alleviate the problem of large state spaces of
the Markov Decision Processes (MDPs) that underlay many practical environments. This new method leverages the characteristic of
many control problems of being composed of many almost independent sub-problems that only seldom interact with each other. The
method considers these sub-problems independently as well as combining them to form a global agent that controls the whole system
and takes into account their interactions. 

\paragraph{}
This project deals with developing reinforcement learning algorithms that use the high-level idea of state-decomposition and to
test and benchmark them against baseline algorithms to determine if such methodology can provide any performance gains. The
performance is measured in terms of training samples or episodes, which is the amount of training experience required to achieve a
certain level of accuracy by the algorithm. Although training time is usually related to the number of training samples used, this
work does not directly consider the reduction of training time (or computational effort).

\section{Structure of the report}

\paragraph{}
This project's report has the following structure. Chapter 2 provides the background information required to understand the topic
of this project. This is about reinforcement learning basics, neural networks, Q-learning and Deep Q-learning, as well as introducing the
problem of large state spaces which this project tries to alleviate and methodologies used in the literature to address the issue.
Chapter 3 introduces the state-decomposition method more specifically, explaining the high-level ideas and the originally devised
state-decomposition method. Chapter 4 presents the tools and methods that are used in testing the state-decomposition technique
and the type of experiments that are run. Chapter 5 shows the most important results obtained during testing and their
interpretation. Chapter 6 discusses the applicability of the proposed techniques and provides a real-life example where the
technique can be useful. Lastly, Chapter 7 discusses the findings of this project, its limitations and possible extensions.
