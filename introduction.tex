\chapter{Introduction}

\section{Motivation}

%The online nature of this family of algorithms causes them to focus on learning optimal decisions in the most visited states at
%the expense of the least visited ones. $

\paragraph{}
Reinforcement learning (RL) has been a very active research area in the last few years, achieving impressive feats in a wide range
of applications. Some advantages of reinforcement learning algorithms are great flexibility, the capability of solving both very
low and high-level problems \cite{sutton_barto_2018} and not requiring a model of the environment in which they are used. RL has
famously been used to solve games, such DeepMind's AlphaGo \cite{silver_mastering_2017}, a program that in 2016 beat the
top-ranked Go player in the world. Other successful applications are in healthcare \cite{yu_reinforcement_2020}, robotics
\cite{noauthor_google_nodate}, autonomous driving \cite{autonomousDriving} and natural language processing \cite{NLP}. My project
supervisor Prof. K. K. Leung has also been working on applications of RL in software-defined networks (SDNs), such as for the
control of the placement of services \cite{ServicePlacement} and the synchronisation of controllers in distributed SDNs
\cite{SDNSynchronisation}.

\paragraph{}
Dealing with very large systems is one of the main problems encountered in the application of reinforcement learning in practical
environments. RL is based on learning through interaction with the environment. As the size of the system increases, so does the
complexity of the problem and it becomes increasingly difficult to learn how to interact with the environment, making the learning
process longer. In practical scenarios, it might not be feasible to train an RL agent for long enough for it to be a feasible
solution, as this might incur excessive costs or, in the case that the behaviour of the environment changes over time, the
algorithm might not be able to learn fast enough to adapt to the changing environment.

\section{Project Definition}

\paragraph{}
The goal of this project is to develop a state-decomposition method, that aims to alleviate the problem of large state spaces of
the Markov Decision Processes (MDPs) that underlay many practical environments. This new method leverages the characteristic of
many control problems of being composed of many almost independent sub-problems that only seldom interact with each other. The
method considers these sub-problems independently as well as combining them to form a global agent that controls the whole system
and takes into account their interactions. 

\paragraph{}
This project deals with developing reinforcement learning algorithms that use the high-level idea of state-decomposition and to
test them and benchmark them against baseline algorithms to determine if such methodology can provide any performance gains. The
performance is measured in terms of training samples or training episodes, that is the amount of training experience required to
achieve a certain level of accuracy by the algorithm. This is not to be confused with reducing training time, which is more
involved with reducing computational effort, which was not considered in this project.

\section{Structure of the report}

\paragraph{}
This project has the following structure. Chapter-2 provides the background information required to understand the topic of this
project. This is about reinforcement learning basics and Q-learning and Deep Q-learning, as well as introducing the problem of
large state-spaces which this project tries to alleviate and methodologies used in the literature for the same scope. Chapter-3
introduces the state-decomposition method more specifically, explaining the high-level ideas and the originally devised state-decomposition method. Chapter-4 presents the tools and methods that were used in testing the state-decomposition technique and
the type of experiments that were run. Chapter-5 shows the most important results obtained during testing and their
interpretation. Chapter-6 discusses the applicability of the proposed techniques and provides a real-life example where the
technique could be useful. Lastly, Chapter-7 discusses the findings of this project, its limitation and possible extensions..
